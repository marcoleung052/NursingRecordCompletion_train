import pandas as pd
import os
import re
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import torch
from torch.utils.data import DataLoader
import json
from tqdm import tqdm
import warnings
import gc
warnings.filterwarnings("ignore")

class MIMICNursingTextCompletion:
    def __init__(self, model_name = "Qwen/Qwen3-0.6B", output_dir="./nursing_model"):
        """
        åˆå§‹åŒ–MIMIC-IIIè­·ç†è¨˜éŒ„æ–‡å­—è£œé½Šæ¨¡å‹
        """
        self.model_name = model_name
        self.output_dir = output_dir
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.test_dataset = None
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs("data", exist_ok=True)
    
    def preprocess_nursing_data(self, data_path="data/NOTEEVENTS.csv", max_samples=5000):
        """
        æ¥µåº¦å„ªåŒ–çš„è³‡æ–™é è™•ç† - å¤§å¹…æ¸›å°‘æ¨£æœ¬æ•¸
        """
        print(f"ğŸ”„ é–‹å§‹ç¯©é¸ Nursing é¡åˆ¥... (æ¥µé™å„ªåŒ–ï¼Œæœ€å¤š {max_samples} ç­†)")
        
        chunksize = 5000  # é€²ä¸€æ­¥æ¸›å°‘chunk size
        nursing_texts = []
        total_rows = 0
        
        try:
            for chunk in pd.read_csv(data_path, chunksize=chunksize, low_memory=False):
                total_rows += len(chunk)
                nursing_df = chunk[chunk["CATEGORY"] == "Nursing"]
                
                # æ¸…ç†å’Œæ¨™æº–åŒ–æ–‡æœ¬
                clean_texts = self._clean_nursing_texts(nursing_df["TEXT"].astype(str).tolist())
                nursing_texts.extend(clean_texts)
                
                print(f"å·²è™•ç† {total_rows} è¡Œï¼Œæ”¶é›†åˆ° {len(nursing_texts)} ç­†è­·ç†è¨˜éŒ„")
                
                # ææ—©åœæ­¢
                if len(nursing_texts) >= max_samples:
                    nursing_texts = nursing_texts[:max_samples]
                    print(f"ğŸ›‘ å·²é”åˆ°æœ€å¤§æ¨£æœ¬æ•¸ {max_samples}ï¼Œåœæ­¢è®€å–")
                    break
                
                # å¼·åˆ¶åƒåœ¾å›æ”¶
                del chunk, nursing_df
                gc.collect()
                
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {data_path}")
            return False
        
        if len(nursing_texts) == 0:
            print("âŒ æ²’æœ‰æ‰¾åˆ°è­·ç†è¨˜éŒ„")
            return False
            
        print(f"âœ… æ”¶é›†åˆ° {len(nursing_texts)} ç­† Nursing è¨˜éŒ„")
        
        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™ (8:2)
        train_texts, test_texts = train_test_split(
            nursing_texts, 
            test_size=0.2,
            random_state=42
        )
        
        print(f"ğŸ“Š è¨“ç·´è³‡æ–™: {len(train_texts)} ç­†")
        print(f"ğŸ“Š æ¸¬è©¦è³‡æ–™: {len(test_texts)} ç­†")
        
        # å‰µå»ºæ¥µå°‘é‡çš„è¨“ç·´æ¨£æœ¬
        self._create_minimal_samples(train_texts, test_texts)
        
        # æ¸…ç†è¨˜æ†¶é«”
        del nursing_texts, train_texts, test_texts
        gc.collect()
        
        return True
    
    def _clean_nursing_texts(self, texts):
        """æ¸…ç†è­·ç†è¨˜éŒ„æ–‡æœ¬"""
        cleaned_texts = []
        
        for text in texts:
            if pd.isna(text) or len(str(text).strip()) < 30:
                continue
                
            text = str(text).strip()
            text = re.sub(r'\s+', ' ', text)  # æ¨™æº–åŒ–ç©ºç™½
            text = re.sub(r'\n+', ' ', text)  # æ›è¡Œæ”¹ç‚ºç©ºç™½
            
            # åªå–ä¸­ç­‰é•·åº¦çš„æ–‡æœ¬
            if 50 < len(text) < 500:
                cleaned_texts.append(text)
                
        return cleaned_texts
    
    def _create_minimal_samples(self, train_texts, test_texts):
        """å‰µå»ºæ¥µå°‘é‡çš„è¨“ç·´æ¨£æœ¬"""
        print("ğŸ”„ å‰µå»ºæ¥µå°‘é‡è¨“ç·´æ¨£æœ¬...")
        
        def create_samples(texts, split_name, max_total_samples=1000):
            samples = []
            sample_count = 0
            
            for text in tqdm(texts[:min(len(texts), 500)], desc=f"è™•ç†{split_name}è³‡æ–™"):  # æœ€å¤šè™•ç†500ç­†åŸæ–‡
                if sample_count >= max_total_samples:
                    break
                    
                # ç°¡å–®åˆ†å¥
                sentences = re.split(r'[.!?]\s+', text)
                
                for sentence in sentences[:2]:  # æ¯ç¯‡æ–‡ç« æœ€å¤šå–2å¥
                    if sample_count >= max_total_samples:
                        break
                        
                    sentence = sentence.strip()
                    if len(sentence) < 20:
                        continue
                        
                    words = sentence.split()
                    if len(words) < 5:
                        continue
                    
                    # åªå‰µå»ºä¸€å€‹æ¨£æœ¬ï¼šå–å‰ä¸€åŠä½œç‚ºinputï¼Œå¾Œä¸€åŠä½œç‚ºoutput
                    mid_point = len(words) // 2
                    if mid_point >= 2:
                        prefix = ' '.join(words[:mid_point])
                        completion = ' '.join(words[mid_point:])
                        
                        if len(completion.strip()) > 3:
                            samples.append({
                                'input': f"è«‹è£œé½Šä»¥ä¸‹è­·ç†è¨˜éŒ„ï¼š{prefix}",
                                'output': completion
                            })
                            sample_count += 1
            
            return samples
        
        # è¨“ç·´é›†æœ€å¤š1000å€‹æ¨£æœ¬ï¼Œæ¸¬è©¦é›†æœ€å¤š100å€‹æ¨£æœ¬
        train_samples = create_samples(train_texts, "è¨“ç·´", max_total_samples=40000)
        test_samples = create_samples(test_texts, "æ¸¬è©¦", max_total_samples=10000)
        
        # å„²å­˜æ¨£æœ¬
        with open("data/train_samples.json", "w", encoding="utf-8") as f:
            json.dump(train_samples, f, ensure_ascii=False, indent=2)
            
        with open("data/test_samples.json", "w", encoding="utf-8") as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)
        
        print(f"è¨“ç·´æ¨£æœ¬: {len(train_samples)} å€‹")
        print(f"æ¸¬è©¦æ¨£æœ¬: {len(test_samples)} å€‹")
        
        # æ¸…ç†è¨˜æ†¶é«”
        del train_samples, test_samples
        gc.collect()
    
    def load_model_and_tokenizer(self):
        """è¼‰å…¥æ¨¡å‹å’Œtokenizer"""
        print("ğŸ”„ è¼‰å…¥Qwenæ¨¡å‹å’Œtokenizer...")
        
        try:
            # å…ˆè¼‰å…¥tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è¼‰å…¥æ¨¡å‹ (æ›´å¤šè¨˜æ†¶é«”å„ªåŒ–)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                use_cache=False  # æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
            )
            
            print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def prepare_datasets_streaming(self):
        """ä¸²æµæ–¹å¼æº–å‚™è³‡æ–™é›†ï¼Œé¿å…ä¸€æ¬¡æ€§è¼‰å…¥æ‰€æœ‰è³‡æ–™"""
        print("ğŸ”„ ä»¥ä¸²æµæ–¹å¼æº–å‚™è³‡æ–™é›†...")
        
        # è¼‰å…¥æ¨£æœ¬
        with open("data/train_samples.json", "r", encoding="utf-8") as f:
            train_samples = json.load(f)
            
        with open("data/test_samples.json", "r", encoding="utf-8") as f:
            test_samples = json.load(f)
        
        print(f"ğŸ“Š è¼‰å…¥æ¨£æœ¬ - è¨“ç·´: {len(train_samples)}, æ¸¬è©¦: {len(test_samples)}")
        
        # é€ä¸€tokenizeï¼Œè€Œä¸æ˜¯æ‰¹é‡è™•ç†
        def tokenize_samples(samples, desc):
            tokenized_samples = []
            
            for sample in tqdm(samples, desc=desc):
                full_text = f"{sample['input']} {sample['output']}{self.tokenizer.eos_token}"
                
                # å–®å€‹æ¨£æœ¬tokenize
                tokenized = self.tokenizer(
                    full_text,
                    truncation=True,
                    padding=False,  # ä¸paddingï¼Œè¨“ç·´æ™‚å†è™•ç†
                    max_length=128,  # å¤§å¹…æ¸›å°‘max_length
                    return_tensors=None  # è¿”å›åˆ—è¡¨è€Œä¸æ˜¯å¼µé‡
                )
                
                tokenized_samples.append({
                    'input_ids': tokenized['input_ids'],
                    'attention_mask': tokenized['attention_mask'],
                    'labels': tokenized['input_ids'].copy()  # labelså’Œinput_idsç›¸åŒ
                })
                
                # å®šæœŸåƒåœ¾å›æ”¶
                if len(tokenized_samples) % 100 == 0:
                    gc.collect()
            
            return tokenized_samples
        
        # ä¸²æµtokenize
        train_tokenized = tokenize_samples(train_samples, "Tokenizingè¨“ç·´è³‡æ–™")
        test_tokenized = tokenize_samples(test_samples, "Tokenizingæ¸¬è©¦è³‡æ–™")
        
        # è½‰æ›ç‚ºDataset
        self.train_dataset = Dataset.from_list(train_tokenized)
        self.test_dataset = Dataset.from_list(test_tokenized)
        
        print(f"âœ… è³‡æ–™é›†æº–å‚™å®Œæˆ - è¨“ç·´: {len(self.train_dataset)}, æ¸¬è©¦: {len(self.test_dataset)}")
        
        # æ¸…ç†è¨˜æ†¶é«”
        del train_samples, test_samples, train_tokenized, test_tokenized
        gc.collect()
    
    def train_model(self, num_epochs=1, batch_size=1, learning_rate=5e-6):
        """æ¥µç°¡è¨“ç·´é…ç½®"""
        print("ğŸ”„ é–‹å§‹æ¥µç°¡è¨“ç·´...")
        
        # æª¢æŸ¥transformersç‰ˆæœ¬ä¸¦èª¿æ•´åƒæ•¸
        import transformers
        transformers_version = transformers.__version__
        print(f"ğŸ“¦ Transformersç‰ˆæœ¬: {transformers_version}")
        
        # æœ€å°åŒ–è¨“ç·´åƒæ•¸ - ç§»é™¤äº†ä¸ç›¸å®¹çš„åƒæ•¸
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            overwrite_output_dir=True,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=16,  # å¤§å¹…å¢åŠ æ¢¯åº¦ç´¯ç©
            warmup_steps=10,
            learning_rate=learning_rate,
            logging_steps=20,
            save_steps=200,
            save_total_limit=1,
            load_best_model_at_end=False,  # ä¸è¼‰å…¥æœ€ä½³æ¨¡å‹ä»¥ç¯€çœè¨˜æ†¶é«”
            report_to=None,
            dataloader_pin_memory=False,
            fp16=False,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            # ç§»é™¤äº† predict_with_generate åƒæ•¸
        )
        
        # è‡ªå®šç¾©è³‡æ–™collatorï¼Œè™•ç†padding
        def custom_data_collator(features):
            # æ‰¾åˆ°æœ€é•·åºåˆ—
            max_length = max(len(f['input_ids']) for f in features)
            max_length = min(max_length, 128)  # é™åˆ¶æœ€å¤§é•·åº¦
            
            batch = {}
            batch['input_ids'] = []
            batch['attention_mask'] = []
            batch['labels'] = []
            
            for feature in features:
                input_ids = feature['input_ids'][:max_length]
                attention_mask = feature['attention_mask'][:max_length]
                labels = feature['labels'][:max_length]
                
                # Padding
                pad_length = max_length - len(input_ids)
                if pad_length > 0:
                    input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_length
                    attention_mask = attention_mask + [0] * pad_length
                    labels = labels + [-100] * pad_length  # -100æœƒè¢«å¿½ç•¥
                
                batch['input_ids'].append(input_ids)
                batch['attention_mask'].append(attention_mask)
                batch['labels'].append(labels)
            
            # è½‰æ›ç‚ºå¼µé‡
            return {
                'input_ids': torch.tensor(batch['input_ids'], dtype=torch.long),
                'attention_mask': torch.tensor(batch['attention_mask'], dtype=torch.long),
                'labels': torch.tensor(batch['labels'], dtype=torch.long)
            }
        
        # åˆå§‹åŒ–Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.test_dataset,
            data_collator=custom_data_collator,
            tokenizer=self.tokenizer,
        )
        
        # è¨“ç·´
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        try:
            trainer.train()
            print("âœ… è¨“ç·´æˆåŠŸå®Œæˆ")
        except Exception as e:
            print(f"âŒ è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            print("å˜—è©¦é™ä½batch sizeæˆ–èª¿æ•´å…¶ä»–åƒæ•¸")
            raise e
        
        # å„²å­˜æ¨¡å‹
        print("ğŸ’¾ å„²å­˜æ¨¡å‹...")
        try:
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            print("âœ… æ¨¡å‹å„²å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹å„²å­˜å¤±æ•—: {e}")
        
        # æ¸…ç†
        del trainer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def evaluate_model(self):
        """ç°¡å–®è©•ä¼°"""
        print("ğŸ”„ ç°¡å–®è©•ä¼°...")
        
        # è¼‰å…¥å°‘é‡æ¸¬è©¦æ¨£æœ¬
        with open("data/test_samples.json", "r", encoding="utf-8") as f:
            test_samples = json.load(f)
        
        # åªæ¸¬è©¦5å€‹æ¨£æœ¬
        results = []
        for i, sample in enumerate(test_samples[:5]):
            input_text = sample['input']
            expected_output = sample['output']
            predicted_output = self.generate_completion(input_text)
            
            results.append({
                'input': input_text,
                'expected': expected_output,
                'predicted': predicted_output
            })
            
            print(f"æ¸¬è©¦ {i+1}:")
            print(f"è¼¸å…¥: {input_text}")
            print(f"é æœŸ: {expected_output}")
            print(f"é æ¸¬: {predicted_output}")
            print("-" * 40)
        
        return results
    
    def generate_completion(self, input_text, max_length=30):
        """ç”Ÿæˆæ–‡å­—è£œé½Š"""
        if self.model is None:
            return "æ¨¡å‹å°šæœªè¼‰å…¥"
        
        try:
            inputs = self.tokenizer(input_text, return_tensors="pt")
            
            # å°‡è¼¸å…¥ç§»åˆ°æ­£ç¢ºçš„è¨­å‚™
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + max_length,
                    num_return_sequences=1,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    max_new_tokens=max_length  # æ›´æ˜ç¢ºçš„åƒæ•¸
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            completion = generated_text[len(input_text):].strip()
            
            return completion
        
        except Exception as e:
            print(f"ç”Ÿæˆæ™‚å‡ºç¾éŒ¯èª¤: {e}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}"

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸ - æ¥µç°¡ç‰ˆæœ¬"""
    print("ğŸ¥ MIMIC-III è­·ç†è¨˜éŒ„æ–‡å­—è£œé½Šç³»çµ± (æ¥µç°¡ç‰ˆ v2)")
    print("=" * 50)
    
    # æª¢æŸ¥ç³»çµ±è³‡è¨Š
    print(f"ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ”§ CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ”§ CUDAè¨­å‚™: {torch.cuda.get_device_name()}")
        print(f"ğŸ”§ CUDAè¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # è¨­å®šæ¥µå°çš„è³‡æºä½¿ç”¨
    system = MIMICNursingTextCompletion(
        model_name = "Qwen/Qwen3-0.6B",
        output_dir="./nursing_minimal_model"
    )
    
    try:
        # æ­¥é©Ÿ1: æ¥µç°¡è³‡æ–™é è™•ç†
        print("\nğŸ”„ æ­¥é©Ÿ1: è³‡æ–™é è™•ç†")
        if not system.preprocess_nursing_data(max_samples=50000):  # é€²ä¸€æ­¥æ¸›å°‘åˆ°1000ç­†
            print("âŒ è³‡æ–™é è™•ç†å¤±æ•—")
            return
        
        gc.collect()
        
        # æ­¥é©Ÿ2: è¼‰å…¥æ¨¡å‹
        print("\nğŸ”„ æ­¥é©Ÿ2: è¼‰å…¥æ¨¡å‹")
        if not system.load_model_and_tokenizer():
            print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
            return
        
        # æ­¥é©Ÿ3: ä¸²æµæº–å‚™è³‡æ–™é›†
        print("\nğŸ”„ æ­¥é©Ÿ3: æº–å‚™è³‡æ–™é›†")
        system.prepare_datasets_streaming()
        
        # æ­¥é©Ÿ4: æ¥µç°¡è¨“ç·´
        print("\nğŸ”„ æ­¥é©Ÿ4: é–‹å§‹è¨“ç·´")
        system.train_model(num_epochs=1, batch_size=1, learning_rate=5e-6)
        
        # æ­¥é©Ÿ5: ç°¡å–®è©•ä¼°
        print("\nğŸ”„ æ­¥é©Ÿ5: è©•ä¼°çµæœ")
        system.evaluate_model()
        
        print("\nğŸ‰ æ¥µç°¡ç‰ˆè¨“ç·´å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹å„²å­˜åœ¨: {system.output_dir}")
        
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥ç³»çµ±è³‡æºå’Œæ•¸æ“šæª”æ¡ˆ")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

'''
=== ğŸ“Š è©•ä¼°çµæœ ===
ROUGE åˆ†æ•¸ï¼š
rouge1: 0.1485
rouge2: 0.1300
rougeL: 0.1489
rougeLsum: 0.1479

BERTScore (å¹³å‡)ï¼š
Precision: 0.7867
Recall:    0.9087
F1:        0.8431
'''
