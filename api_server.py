# api_server.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# =================================================================
# 1. æ‡‰ç”¨ç¨‹å¼åˆå§‹åŒ–èˆ‡æ¨¡å‹è¼‰å…¥
# =================================================================
app = FastAPI(title="GPT-2 Nursing Completion API")

# è¨­ç½® CORSï¼šå…è¨±å‰ç«¯é é¢ (localhost æˆ–æ‚¨çš„æœå‹™å™¨ IP) è¨ªå•
# âš ï¸ æ³¨æ„ï¼šåœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œè«‹å°‡ "http://localhost:5500" æ›¿æ›ç‚ºæ‚¨çš„å‰ç«¯åŸŸåï¼
origins = [
    "http://localhost:5500",  # å‡è¨­æ‚¨ä½¿ç”¨ VS Code Live Server æˆ–é¡ä¼¼å·¥å…·
    "http://127.0.0.1:5500",
    "*" # ç‚ºäº†æ¸¬è©¦æ–¹ä¾¿ï¼Œæš«æ™‚å…è¨±æ‰€æœ‰ä¾†æº
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€è®Šæ•¸ç”¨æ–¼å­˜å„²æ¨¡å‹å’Œåˆ†è©å™¨
tokenizer = None
model = None
MODEL_PATH = "gpt2" # é€™è£¡å¯ä»¥æ›¿æ›ç‚ºæ‚¨å¾®èª¿å¾Œçš„æ¨¡å‹è³‡æ–™å¤¾è·¯å¾‘

@app.on_event("startup")
async def load_model():
    """åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚è¼‰å…¥ GPT-2 æ¨¡å‹"""
    global tokenizer, model
    try:
        # è¼‰å…¥åˆ†è©å™¨
        tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH) 
        
        # è¼‰å…¥é è¨“ç·´æ¨¡å‹æˆ–æ‚¨å¾®èª¿çš„æ¨¡å‹æ¬Šé‡
        # å¦‚æœæ‚¨çš„è¨˜æ†¶é«”å…è¨±ï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨ GPU
        # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)
        # model.to(device)
        model.eval() # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
        
        print(f"âœ… GPT-2 æ¨¡å‹ {MODEL_PATH} è¼‰å…¥æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ MODEL_PATH æˆ–ä¾è³´åº«æ˜¯å¦å®‰è£ï¼š{e}")


# =================================================================
# 2. API è«‹æ±‚èˆ‡éŸ¿æ‡‰æ ¼å¼
# =================================================================
class PredictionRequest(BaseModel):
    """å‰ç«¯ç™¼é€çš„è«‹æ±‚é«”æ ¼å¼"""
    prompt: str
    patient_id: str | None = None
    model: str | None = "gpt2-nursing"

class PredictionResponse(BaseModel):
    """å¾Œç«¯å›å‚³çš„éŸ¿æ‡‰é«”æ ¼å¼"""
    completions: list[str]

# =================================================================
# 3. æ ¸å¿ƒ API ç«¯é» (å·²ä¿®æ”¹ç‚ºç”Ÿæˆ 3 å€‹åºåˆ—)
# =================================================================
@app.post("/api/predict", response_model=PredictionResponse)
def predict_completion(request: PredictionRequest):
    """æ ¹æ“šè¼¸å…¥æç¤ºè©ç”Ÿæˆ DART è­·ç†ç´€éŒ„"""
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="AI æ¨¡å‹æœå‹™å°šæœªæº–å‚™å°±ç·’ï¼Œè«‹æª¢æŸ¥å¾Œç«¯æ—¥èªŒã€‚")
    
    input_text = request.prompt
    if len(input_text) > 512:
        raise HTTPException(status_code=400, detail="è¼¸å…¥éé•·ï¼Œè«‹é™åˆ¶åœ¨ 512 å€‹å­—å…ƒå…§ã€‚")

    try:
        input_ids = tokenizer.encode(input_text, return_tensors='pt', truncation=True)
        
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè¨­ç½® num_return_sequences=3 ä¾†ç”Ÿæˆå¤šå€‹å€™é¸çµæœ
        output = model.generate(
            input_ids, 
            max_length=len(input_text) + 150,
            num_return_sequences=3,            # <--- è¼¸å‡º 3 å€‹ä¸åŒçš„è£œå…¨çµæœ
            no_repeat_ngram_size=3,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id 
        )
        
        all_completions = []
        for sequence in output:
            generated_text = tokenizer.decode(sequence, skip_special_tokens=True)
            
            # ç¢ºä¿å…§å®¹ä»¥ç”¨æˆ¶çš„è¼¸å…¥ç‚ºé–‹é ­
            if generated_text.startswith(input_text):
                all_completions.append(generated_text)
            
        # ç§»é™¤é‡è¤‡çš„çµæœä¸¦æŒ‰é•·åº¦æ’åº
        unique_completions = sorted(list(set(all_completions)), key=len, reverse=True)

        if not unique_completions:
             # å¦‚æœæ¨¡å‹æ²’æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„è£œå…¨ï¼Œå‰‡è¿”å›ç”¨æˆ¶è¼¸å…¥æœ¬èº«
             return {"completions": [input_text]}

        # è¿”å›æ‰€æœ‰å”¯ä¸€çš„è£œå…¨çµæœ (æœ€å¤š 3 å€‹)
        return {"completions": unique_completions}
        
    except Exception as e:
        print(f"æ¨è«–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹æ¨è«–å¤±æ•—ï¼š{str(e)[:50]}...")

# é‹è¡Œä¼ºæœå™¨
if __name__ == "__main__":
    import uvicorn
    # host 0.0.0.0 å…è¨±å¤–éƒ¨è¨ªå•ï¼Œport 8000 èˆ‡å‰ç«¯è¨­å®šä¸€è‡´
    uvicorn.run("api_server:app", host="0.0.0.0", port=8000, reload=True)